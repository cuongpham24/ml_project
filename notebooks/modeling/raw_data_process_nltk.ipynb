{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "DATASET = [\"meta_Beauty_and_Personal_Care\", \"meta_Books\", \"meta_Home_and_Kitchen\"]\n",
    "LABELS = [\"personal_care\", \"book\", \"home\"]\n",
    "COLUMN_SELECTIONS = [\"main_category\", \"title\", \"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999754\n",
      "5400431\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for label in LABELS:\n",
    "    # Read the extracted raw data\n",
    "    df_temp = pd.read_parquet(f\"../../data/raw/{label}.parquet\")\n",
    "    df_temp.title = df_temp.title.astype(\"str\")\n",
    "    df_temp.features = df_temp.features.astype(\"str\")\n",
    "    # Convert text to lower case\n",
    "    df_temp = df_temp.drop(\"main_category\", axis=1).apply(lambda x: x.str.lower())\n",
    "    # Append non-empty string to documents\n",
    "    documents += df_temp.title[df_temp.title != \"\"].to_list()\n",
    "    print(len(documents))\n",
    "    documents += df_temp.features[df_temp.features != \"\"].to_list()\n",
    "    print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(text):\n",
    "    # Punctuations are replaced by .\n",
    "    text = re.sub(r\"[,!?;]\", \".\", text)\n",
    "    # Tokenize string to words\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # # Drop non-alphabetical tokens\n",
    "    # text = [t for t in text if t.isalpha() or t == \".\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Setup backend device\n",
    "if torch.cuda.is_available():\n",
    "    # Check for CUDA (traditional GPUs)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Check for MPS (Apple Silicon GPUs)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch is using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tokenized_docs.append(tokenize_string(doc))\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"Finish {i} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of each token\n",
    "word_count = Counter(token for doc in tokenized_docs for token in doc)\n",
    "# Select the most common token\n",
    "vocab = word_count.most_common(32000)\n",
    "vocab = {word: i for i, (word, _) in enumerate(vocab)}\n",
    "\n",
    "word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items(), start=2)}\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "word_to_idx[\"<UNK>\"] = 1\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 50\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "context_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context-target pairs for CBOW\n",
    "def create_cbow_pairs(tokenized_docs, context_size):\n",
    "    pairs = []\n",
    "    for doc in tokenized_docs:\n",
    "        if len(doc) < context_size:\n",
    "            continue\n",
    "        for i in range(context_size, len(doc) - context_size):\n",
    "            context = doc[i-context_size : i] + doc[i + 1 : i + context_size + 1]\n",
    "            target = doc[i]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = create_cbow_pairs(tokenized_docs, context_size)\n",
    "\n",
    "# Dataset and Dataloader for CBOW\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_pairs, word_to_idx, context_size):\n",
    "        self.data = [(torch.tensor([word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in context], dtype=torch.long), \n",
    "                      word_to_idx.get(target, word_to_idx[\"<UNK>\"])) for context, target in cbow_pairs]\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return context, torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "dataset = CBOWDataset(cbow_pairs, word_to_idx, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Training Loss : 6.220404624938965\n",
      "Epoch:1 | Training Loss : 3.1281750202178955\n",
      "Epoch:2 | Training Loss : 1.4023734331130981\n",
      "Epoch:3 | Training Loss : 0.6749862432479858\n",
      "Epoch:4 | Training Loss : 0.3959619700908661\n",
      "Epoch:5 | Training Loss : 0.2872588038444519\n",
      "Epoch:6 | Training Loss : 0.2239122837781906\n",
      "Epoch:7 | Training Loss : 0.1773415356874466\n",
      "Epoch:8 | Training Loss : 0.15400497615337372\n",
      "Epoch:9 | Training Loss : 0.13397695124149323\n",
      "Epoch:10 | Training Loss : 0.11774098128080368\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=len(vocab), embedding_dim=256)\n",
    "        self.linear_1 = nn.Linear(in_features=256, out_features=len(vocab))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "\n",
    "# Training Setup\n",
    "model = WordEmbeddings()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(15):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for feature, label in dataloader:\n",
    "        model = model.to(device)\n",
    "        feature = feature.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        y_train_pred = model(feature)\n",
    "\n",
    "        loss = loss_fn(y_train_pred, label)\n",
    "        train_loss = train_loss + loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    print(f\"Epoch:{epoch} | Training Loss : {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.embeddings.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([782, 300])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights = model.embeddings.weight.data.cpu().numpy()\n",
    "np.save(\"embedding_weights.npy\", embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset and Dataloader\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, tokenized_docs, word_to_idx, max_len=None):\n",
    "#         self.data = [[word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in doc] for doc in tokenized_docs]\n",
    "#         if max_len:\n",
    "#             self.max_len = max_len\n",
    "#         else:\n",
    "#             self.max_len = max(len(doc) for doc in self.data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         doc = self.data[idx]\n",
    "#         if len(doc) > self.max_len:\n",
    "#             padded_doc = doc[:self.max_len]\n",
    "#         else:\n",
    "#             padded_doc = doc + [word_to_idx[\"<PAD>\"]] * (self.max_len - len(doc))\n",
    "#         return torch.tensor(padded_doc, dtype=torch.long)\n",
    "\n",
    "# dataset = TextDataset(documents, word_to_idx)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
