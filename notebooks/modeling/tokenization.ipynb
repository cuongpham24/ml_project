{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "DATASET = [\"meta_Beauty_and_Personal_Care\", \"meta_Books\", \"meta_Home_and_Kitchen\"]\n",
    "LABELS = [\"personal_care\", \"book\", \"home\"]\n",
    "COLUMN_SELECTIONS = [\"main_category\", \"title\", \"features\"]\n",
    "\n",
    "documents = []\n",
    "for label in LABELS:\n",
    "    # Read the extracted raw data\n",
    "    df_temp = pd.read_parquet(f\"../../data/raw/{label}.parquet\")\n",
    "    df_temp.title = df_temp.title.astype(\"str\")\n",
    "    df_temp.features = df_temp.features.astype(\"str\")\n",
    "    # Convert text to lower case\n",
    "    df_temp = df_temp.drop(\"main_category\", axis=1).apply(lambda x: x.str.lower())\n",
    "    # Append non-empty string to documents\n",
    "    documents += df_temp.title[df_temp.title != \"\"].to_list()[:10000]\n",
    "    # documents += df_temp.features[df_temp.features != \"\"].to_list()\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Setup backend device\n",
    "if torch.cuda.is_available():\n",
    "    # Check for CUDA (traditional GPUs)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Check for MPS (Apple Silicon GPUs)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch is using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 200\n",
    "\n",
    "# Send data to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Function to get embeddings for a batch of texts\n",
    "def get_embeddings(texts_batch):\n",
    "    inputs = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    # inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word to vector\n",
    "from math import ceil\n",
    "\n",
    "num_batch = ceil(len(documents) / batch_size)\n",
    "for i in range(num_batch):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    if i == 0:\n",
    "        xtrain = get_embeddings(documents[i * batch_size:(i + 1) * batch_size])\n",
    "    elif i == num_batch - 1:\n",
    "        xtrain = np.vstack([xtrain, get_embeddings(documents[i * batch_size:len(documents)])])\n",
    "    else:\n",
    "        xtrain = np.vstack([xtrain, get_embeddings(documents[i * batch_size:(i + 1) * batch_size])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy labels\n",
    "ytrain = np.array([0] * 10000 + [1] * 10000 + [2] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmod = LogisticRegression(solver=\"lbfgs\", max_iter=1000).fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = logmod.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9726"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ypred == ytrain).sum() / len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
