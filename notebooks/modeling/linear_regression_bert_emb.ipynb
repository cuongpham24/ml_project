{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from numpy import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999754\n"
     ]
    }
   ],
   "source": [
    "DATASET = [\"meta_Beauty_and_Personal_Care\", \"meta_Books\", \"meta_Home_and_Kitchen\"]\n",
    "LABELS = [\"personal_care\", \"book\", \"home\"]\n",
    "COLUMN_SELECTIONS = [\"main_category\", \"title\", \"features\"]\n",
    "\n",
    "label_to_id = {\"personal_care\": 0, \"book\": 1, \"home\": 2}\n",
    "\n",
    "documents = []\n",
    "ytrain = []\n",
    "for label in LABELS:\n",
    "    # Read the extracted raw data\n",
    "    df_temp = pd.read_parquet(f\"../../data/raw/{label}.parquet\")\n",
    "    df_temp.title = df_temp.title.astype(\"str\")\n",
    "    df_temp.features = df_temp.features.astype(\"str\")\n",
    "    # Convert text to lower case\n",
    "    df_temp = df_temp.drop(\"main_category\", axis=1).apply(lambda x: x.str.lower())\n",
    "    # Append non-empty string to documents\n",
    "    documents += df_temp.title[df_temp.title != \"\"].to_list()\n",
    "    # documents += df_temp.features[df_temp.features != \"\"].to_list()\n",
    "    ytrain += [label_to_id[label]] * len(df_temp.title[df_temp.title != \"\"])\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using MPS.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Setup backend device\n",
    "if torch.cuda.is_available():\n",
    "    # Check for CUDA (traditional GPUs)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Check for MPS (Apple Silicon GPUs)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch is using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 200\n",
    "\n",
    "# Send data to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Function to get embeddings for a batch of texts\n",
    "def get_embeddings(texts_batch):\n",
    "    inputs = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word to vector\n",
    "from math import ceil\n",
    "\n",
    "num_batch = ceil(len(documents) / batch_size)\n",
    "for i in range(num_batch):\n",
    "    if i % 4 == 0:\n",
    "        print(f\"Batch {i+1}\") \n",
    "    if i == 0:\n",
    "        xtrain = get_embeddings(documents[i * batch_size:(i + 1) * batch_size])\n",
    "    elif i == num_batch - 1:\n",
    "        xtrain = np.vstack([xtrain, get_embeddings(documents[i * batch_size:len(documents)])])\n",
    "    else:\n",
    "        xtrain = np.vstack([xtrain, get_embeddings(documents[i * batch_size:(i + 1) * batch_size])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod = LogisticRegression(solver=\"lbfgs\", max_iter=1000).fit(xtrain, ytrain)\n",
    "\n",
    "ypred = logmod.predict(xtrain)\n",
    "\n",
    "print(accuracy_score(ytrain, ypred))\n",
    "print(confusion_matrix(ytrain, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Extract the embedding layer\n",
    "# embedding_layer = model.embeddings\n",
    "embedding_layer = model.embeddings\n",
    "\n",
    "# Example texts\n",
    "texts = documents\n",
    "\n",
    "# Tokenize texts\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(inputs['input_ids'])\n",
    "\n",
    "# Define batch size and DataLoader\n",
    "batch_size = 200\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Function to get embeddings for a batch of input_ids\n",
    "def get_embeddings(input_ids):\n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_layer(input_ids)\n",
    "        embeddings = embeddings.mean(dim=1)  # Mean pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Process texts in batches using DataLoader\n",
    "all_embeddings = []\n",
    "for batch in dataloader:\n",
    "    input_ids = batch[0]\n",
    "    batch_embeddings = get_embeddings(input_ids)\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "all_embeddings_np = np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save numpy array as npz file\n",
    "# from numpy import asarray\n",
    "# from numpy import savez_compressed\n",
    "\n",
    "# # save to npy file\n",
    "# savez_compressed('data.npz', all_embeddings_np)\n",
    "# savez_compressed('ytrain_label.npz', ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dict of arrays\n",
    "dict_data = load('data.npz')\n",
    "# extract the first array\n",
    "X = dict_data['arr_0']\n",
    "\n",
    "# load dict of arrays\n",
    "dict_data = load('ytrain_label.npz')\n",
    "# extract the first array\n",
    "y = dict_data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=0)\n",
    "\n",
    "for train_idx, test_idx in splitter.split(X, y):\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod = LogisticRegression(solver=\"lbfgs\", max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "# logmod = SGDClassifier(loss=\"log_loss\", n_jobs=-1, max_iter=1000)\n",
    "# batch_size = 100000\n",
    "# num_batch = ceil(len(all_embeddings_np) / batch_size)\n",
    "# for i in range(num_batch):\n",
    "#     print(f\"Batch {i+1}\") \n",
    "#     if i == num_batch - 1:\n",
    "#         logmod.partial_fit(all_embeddings_np[i * batch_size : len(all_embeddings_np)], ytrain[i * batch_size : len(all_embeddings_np)], classes=[0, 1, 2])\n",
    "#     else:\n",
    "#         logmod.partial_fit(all_embeddings_np[i * batch_size : (i + 1) * batch_size], ytrain[i * batch_size : (i + 1) * batch_size], classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = logmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train, ypred))\n",
    "print(confusion_matrix(y_train, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379540995847995\n",
      "[[369205   8040  22734]\n",
      " [  3073 390595   6332]\n",
      " [ 21439  12831 365653]]\n"
     ]
    }
   ],
   "source": [
    "ypred = logmod.predict(X_test)\n",
    "print(accuracy_score(y_test, ypred))\n",
    "print(confusion_matrix(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93    399979\n",
      "           1       0.95      0.98      0.96    400000\n",
      "           2       0.93      0.91      0.92    399923\n",
      "\n",
      "    accuracy                           0.94   1199902\n",
      "   macro avg       0.94      0.94      0.94   1199902\n",
      "weighted avg       0.94      0.94      0.94   1199902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(y_test, ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../src/models/lr_lable_bert_emb_full_60_0.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(logmod, \"../../src/models/lr_lable_bert_emb_full_60_0.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
