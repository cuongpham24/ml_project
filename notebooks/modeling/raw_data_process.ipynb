{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import os\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "DATASET = [\"meta_Beauty_and_Personal_Care\", \"meta_Books\", \"meta_Home_and_Kitchen\"]\n",
    "LABELS = [\"personal_care\", \"book\", \"home\"]\n",
    "COLUMN_SELECTIONS = [\"main_category\", \"title\", \"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400431\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for label in LABELS:\n",
    "    # Read the extracted raw data\n",
    "    df_temp = pd.read_parquet(f\"../../data/raw/{label}.parquet\")\n",
    "    df_temp.title = df_temp.title.astype(\"str\")\n",
    "    df_temp.features = df_temp.features.astype(\"str\")\n",
    "    # Convert text to lower case\n",
    "    df_temp = df_temp.drop(\"main_category\", axis=1).apply(lambda x: x.str.lower())\n",
    "    # Append non-empty string to documents\n",
    "    documents += df_temp.title[df_temp.title != \"\"].to_list()\n",
    "    documents += df_temp.features[df_temp.features != \"\"].to_list()\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using MPS.\n"
     ]
    }
   ],
   "source": [
    "# Setup backend device\n",
    "if torch.cuda.is_available():\n",
    "    # Check for CUDA (traditional GPUs)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Check for MPS (Apple Silicon GPUs)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch is using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 256\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "context_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 0 docs\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for i, doc in enumerate(documents[:1000]):\n",
    "    tokenized_docs.append(tokenizer.tokenize(doc))\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"Finish {i} docs\")\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context-target pairs for CBOW\n",
    "def create_cbow_pairs(tokenized_docs, context_size):\n",
    "    pairs = []\n",
    "    for doc in tokenized_docs:\n",
    "        if len(doc) < context_size:\n",
    "            continue\n",
    "        for i in range(context_size, len(doc) - context_size):\n",
    "            context = doc[i-context_size : i] + doc[i + 1 : i + context_size + 1]\n",
    "            target = doc[i]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = create_cbow_pairs(tokenized_docs, context_size)\n",
    "\n",
    "# Dataset and Dataloader for CBOW\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_pairs, tokenizer, context_size):\n",
    "        self.data = [(torch.tensor(tokenizer.convert_tokens_to_ids(context), dtype=torch.long), \n",
    "                      tokenizer.convert_tokens_to_ids(target)) for context, target in cbow_pairs]\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return context, torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "dataset = CBOWDataset(cbow_pairs, tokenizer, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim)\n",
    "        self.linear_1 = nn.Linear(in_features=embedding_dim, out_features=len(vocab))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "\n",
    "# Training Setup\n",
    "model = WordEmbeddings()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Training Loss : 1.3389217853546143\n",
      "Epoch:1 | Training Loss : 1.1921583414077759\n",
      "Epoch:2 | Training Loss : 1.0680516958236694\n",
      "Epoch:3 | Training Loss : 0.9571227431297302\n",
      "Epoch:4 | Training Loss : 0.8656147718429565\n",
      "Epoch:5 | Training Loss : 0.78424471616745\n",
      "Epoch:6 | Training Loss : 0.7172321677207947\n",
      "Epoch:7 | Training Loss : 0.6511171460151672\n",
      "Epoch:8 | Training Loss : 0.5987640023231506\n",
      "Epoch:9 | Training Loss : 0.5466225147247314\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for feature, label in dataloader:\n",
    "        model = model.to(device)\n",
    "        feature = feature.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        y_train_pred = model(feature)\n",
    "\n",
    "        loss = loss_fn(y_train_pred, label)\n",
    "        train_loss = train_loss + loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    print(f\"Epoch:{epoch} | Training Loss : {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.embeddings.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.embeddings.weight.data.cpu().numpy()\n",
    "# Save the embedding weights to a .npy file\n",
    "np.save('embedding_weights.npy', embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset and Dataloader\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, tokenized_docs, word_to_idx, max_len=None):\n",
    "#         self.data = [[word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in doc] for doc in tokenized_docs]\n",
    "#         if max_len:\n",
    "#             self.max_len = max_len\n",
    "#         else:\n",
    "#             self.max_len = max(len(doc) for doc in self.data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         doc = self.data[idx]\n",
    "#         if len(doc) > self.max_len:\n",
    "#             padded_doc = doc[:self.max_len]\n",
    "#         else:\n",
    "#             padded_doc = doc + [word_to_idx[\"<PAD>\"]] * (self.max_len - len(doc))\n",
    "#         return torch.tensor(padded_doc, dtype=torch.long)\n",
    "\n",
    "# dataset = TextDataset(documents, word_to_idx)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
