{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import os\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "DATASET = [\"meta_Beauty_and_Personal_Care\", \"meta_Books\", \"meta_Home_and_Kitchen\"]\n",
    "LABELS = [\"personal_care\", \"book\", \"home\"]\n",
    "COLUMN_SELECTIONS = [\"main_category\", \"title\", \"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400431\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for label in LABELS:\n",
    "    # Read the extracted raw data\n",
    "    df_temp = pd.read_parquet(f\"../../data/raw/{label}.parquet\")\n",
    "    df_temp.title = df_temp.title.astype(\"str\")\n",
    "    df_temp.features = df_temp.features.astype(\"str\")\n",
    "    # Convert text to lower case\n",
    "    df_temp = df_temp.drop(\"main_category\", axis=1).apply(lambda x: x.str.lower())\n",
    "    # Append non-empty string to documents\n",
    "    documents += df_temp.title[df_temp.title != \"\"].to_list()\n",
    "    documents += df_temp.features[df_temp.features != \"\"].to_list()\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using MPS.\n"
     ]
    }
   ],
   "source": [
    "# Setup backend device\n",
    "if torch.cuda.is_available():\n",
    "    # Check for CUDA (traditional GPUs)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Check for MPS (Apple Silicon GPUs)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch is using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 256\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "context_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 0 docs\n",
      "Finish 50000 docs\n",
      "Finish 100000 docs\n",
      "Finish 150000 docs\n",
      "Finish 200000 docs\n",
      "Finish 250000 docs\n",
      "Finish 300000 docs\n",
      "Finish 350000 docs\n",
      "Finish 400000 docs\n",
      "Finish 450000 docs\n",
      "Finish 500000 docs\n",
      "Finish 550000 docs\n",
      "Finish 600000 docs\n",
      "Finish 650000 docs\n",
      "Finish 700000 docs\n",
      "Finish 750000 docs\n",
      "Finish 800000 docs\n",
      "Finish 850000 docs\n",
      "Finish 900000 docs\n",
      "Finish 950000 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1000000 docs\n",
      "Finish 1050000 docs\n",
      "Finish 1100000 docs\n",
      "Finish 1150000 docs\n",
      "Finish 1200000 docs\n",
      "Finish 1250000 docs\n",
      "Finish 1300000 docs\n",
      "Finish 1350000 docs\n",
      "Finish 1400000 docs\n",
      "Finish 1450000 docs\n",
      "Finish 1500000 docs\n",
      "Finish 1550000 docs\n",
      "Finish 1600000 docs\n",
      "Finish 1650000 docs\n",
      "Finish 1700000 docs\n",
      "Finish 1750000 docs\n",
      "Finish 1800000 docs\n",
      "Finish 1850000 docs\n",
      "Finish 1900000 docs\n",
      "Finish 1950000 docs\n",
      "Finish 2000000 docs\n",
      "Finish 2050000 docs\n",
      "Finish 2100000 docs\n",
      "Finish 2150000 docs\n",
      "Finish 2200000 docs\n",
      "Finish 2250000 docs\n",
      "Finish 2300000 docs\n",
      "Finish 2350000 docs\n",
      "Finish 2400000 docs\n",
      "Finish 2450000 docs\n",
      "Finish 2500000 docs\n",
      "Finish 2550000 docs\n",
      "Finish 2600000 docs\n",
      "Finish 2650000 docs\n",
      "Finish 2700000 docs\n",
      "Finish 2750000 docs\n",
      "Finish 2800000 docs\n",
      "Finish 2850000 docs\n",
      "Finish 2900000 docs\n",
      "Finish 2950000 docs\n",
      "Finish 3000000 docs\n",
      "Finish 3050000 docs\n",
      "Finish 3100000 docs\n",
      "Finish 3150000 docs\n",
      "Finish 3200000 docs\n",
      "Finish 3250000 docs\n",
      "Finish 3300000 docs\n",
      "Finish 3350000 docs\n",
      "Finish 3400000 docs\n",
      "Finish 3450000 docs\n",
      "Finish 3500000 docs\n",
      "Finish 3550000 docs\n",
      "Finish 3600000 docs\n",
      "Finish 3650000 docs\n",
      "Finish 3700000 docs\n",
      "Finish 3750000 docs\n",
      "Finish 3800000 docs\n",
      "Finish 3850000 docs\n",
      "Finish 3900000 docs\n",
      "Finish 3950000 docs\n",
      "Finish 4000000 docs\n",
      "Finish 4050000 docs\n",
      "Finish 4100000 docs\n",
      "Finish 4150000 docs\n",
      "Finish 4200000 docs\n",
      "Finish 4250000 docs\n",
      "Finish 4300000 docs\n",
      "Finish 4350000 docs\n",
      "Finish 4400000 docs\n",
      "Finish 4450000 docs\n",
      "Finish 4500000 docs\n",
      "Finish 4550000 docs\n",
      "Finish 4600000 docs\n",
      "Finish 4650000 docs\n",
      "Finish 4700000 docs\n",
      "Finish 4750000 docs\n",
      "Finish 4800000 docs\n",
      "Finish 4850000 docs\n",
      "Finish 4900000 docs\n",
      "Finish 4950000 docs\n",
      "Finish 5000000 docs\n",
      "Finish 5050000 docs\n",
      "Finish 5100000 docs\n",
      "Finish 5150000 docs\n",
      "Finish 5200000 docs\n",
      "Finish 5250000 docs\n",
      "Finish 5300000 docs\n",
      "Finish 5350000 docs\n",
      "Finish 5400000 docs\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tokenized_docs.append(tokenizer.tokenize(doc))\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"Finish {i} docs\")\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context-target pairs for CBOW\n",
    "def create_cbow_pairs(tokenized_docs, context_size):\n",
    "    pairs = []\n",
    "    for doc in tokenized_docs:\n",
    "        if len(doc) < context_size:\n",
    "            continue\n",
    "        for i in range(context_size, len(doc) - context_size):\n",
    "            context = doc[i-context_size : i] + doc[i + 1 : i + context_size + 1]\n",
    "            target = doc[i]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = create_cbow_pairs(tokenized_docs, context_size)\n",
    "\n",
    "# Dataset and Dataloader for CBOW\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_pairs, tokenizer, context_size):\n",
    "        self.data = [(torch.tensor(tokenizer.convert_tokens_to_ids(context), dtype=torch.long), \n",
    "                      tokenizer.convert_tokens_to_ids(target)) for context, target in cbow_pairs]\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return context, torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "dataset = CBOWDataset(cbow_pairs, tokenizer, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim)\n",
    "        self.linear_1 = nn.Linear(in_features=embedding_dim, out_features=len(vocab))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "\n",
    "# Training Setup\n",
    "model = WordEmbeddings()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for feature, label in dataloader:\n",
    "        model = model.to(device)\n",
    "        feature = feature.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        y_train_pred = model(feature)\n",
    "\n",
    "        loss = loss_fn(y_train_pred, label)\n",
    "        train_loss = train_loss + loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    print(f\"Epoch:{epoch} | Training Loss : {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.embeddings.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.embeddings.weight.data.cpu().numpy()\n",
    "# Save the embedding weights to a .npy file\n",
    "np.save('embedding_weights.npy', embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset and Dataloader\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, tokenized_docs, word_to_idx, max_len=None):\n",
    "#         self.data = [[word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in doc] for doc in tokenized_docs]\n",
    "#         if max_len:\n",
    "#             self.max_len = max_len\n",
    "#         else:\n",
    "#             self.max_len = max(len(doc) for doc in self.data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         doc = self.data[idx]\n",
    "#         if len(doc) > self.max_len:\n",
    "#             padded_doc = doc[:self.max_len]\n",
    "#         else:\n",
    "#             padded_doc = doc + [word_to_idx[\"<PAD>\"]] * (self.max_len - len(doc))\n",
    "#         return torch.tensor(padded_doc, dtype=torch.long)\n",
    "\n",
    "# dataset = TextDataset(documents, word_to_idx)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
