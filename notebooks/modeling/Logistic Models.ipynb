{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading original data extraction into a data frame for embedding\n",
    "import pandas as pd\n",
    "parquet_files=['book.parquet','home.parquet','personal_care.parquet']\n",
    "dataframes=[pd.read_parquet(files) for files in parquet_files]\n",
    "df=pd.concat(dataframes,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labeling the response column with numerical value\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "df['response']=label_encoder.fit_transform(df['main_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing all numbers in the title columns: \n",
    "import re\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "def remove_num(text):\n",
    "    text=remove_punctuation(text)\n",
    "    return re.sub(r'\\d+','',text)\n",
    "\n",
    "#df['title']= df['title'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to lemmatize text - remove stop words\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting using TF-IDF\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tdif=TfidfVectorizer(stop_words='english', preprocessor=lemmatize_text)  ## we can customize stop_words list as well . using default stop_words='english' has very minimal list. Instead we process stop_words and remove it using spacy  \n",
    "#X=tdif.fit_transform(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl.gz']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## saving the vectorizer and resulting tf-dif matrix to a compressed file for future use. \n",
    "#from scipy.sparse import save_npz, load_npz\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import joblib\n",
    "\n",
    "#joblib.dump((tdif,X), 'tfidf_vectorizer.pkl.gz',compress=('gzip',3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AASLH' 'AASMComfort' 'AASVOG' 'AAT' 'AATBS' 'AATKRMFFV' 'AATMANA'\n",
      " 'AATQRFV' 'AATwengeDesk' 'AAU' 'AAUS' 'AAUSB' 'AAUU' 'AAUW' 'AAVBO'\n",
      " 'AAVIX' 'AAVMDFBS' 'AAVNI' 'AAVRANI' 'AAVandegriftUSMC' 'AAW' 'AAWD'\n",
      " 'AAWESGS' 'AAXplosion' 'AAYU' 'AAZZEUSAM' 'AAZZKANG' 'AAbcalet'\n",
      " 'AAkatsuki' 'AAkron' 'AAlmond' 'AAmbi' 'AAmerica' 'AAndrea' 'AAngel'\n",
      " 'AAobosi' 'AArbutin' 'AArm' 'AArt' 'AAsXXX' 'AAtlonia' 'AAtter' 'AAugust'\n",
      " 'AB' 'ABA' 'ABAAARP' 'ABABA' 'ABACAD' 'ABACUS']\n"
     ]
    }
   ],
   "source": [
    "#print(tdif.get_feature_names_out()[101:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the vectorizer from disk \n",
    "import joblib\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "v,X=joblib.load('tfidf_vectorizer.pkl.gz')\n",
    "\n",
    "# Testing with new text data using the loaded vectorizer\n",
    "new_texts = [\"New data to transform.\"]\n",
    "new_tfidf_matrix = v.transform(new_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape  (6000000, 813248)\n"
     ]
    }
   ],
   "source": [
    "print(\"matrix shape \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression with 1 variable 'Title' - X is the embedding for title column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,df['response'],train_size=.8,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logmod=LogisticRegressionCV(multi_class='multinomial',cv=5).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9730125\n",
      "[[1575024   13430   11380]\n",
      " [  30709  746445   22543]\n",
      " [  28863   22615 2348991]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "ytrain_pred=logmod.predict(x_train)\n",
    "log_train_matrix=confusion_matrix(y_train,ytrain_pred)\n",
    "print('train accuracy', accuracy_score(y_train,ytrain_pred))\n",
    "print(log_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97   1599834\n",
      "           1       0.95      0.93      0.94    799697\n",
      "           2       0.99      0.98      0.98   2400469\n",
      "\n",
      "    accuracy                           0.97   4800000\n",
      "   macro avg       0.97      0.97      0.97   4800000\n",
      "weighted avg       0.97      0.97      0.97   4800000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,ytrain_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.972255\n",
      "[[393802   3382   2982]\n",
      " [  8032 186479   5792]\n",
      " [  7296   5810 586425]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    400166\n",
      "           1       0.95      0.93      0.94    200303\n",
      "           2       0.99      0.98      0.98    599531\n",
      "\n",
      "    accuracy                           0.97   1200000\n",
      "   macro avg       0.97      0.96      0.97   1200000\n",
      "weighted avg       0.97      0.97      0.97   1200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytest_pred=logmod.predict(x_test)\n",
    "log_test_matrix=confusion_matrix(y_test,ytest_pred)\n",
    "print(\"test accuracy\",accuracy_score(y_test,ytest_pred))\n",
    "print(log_test_matrix)\n",
    "print(classification_report(y_test,ytest_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
