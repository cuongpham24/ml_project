{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading original data extraction into a data frame for embedding\n",
    "import pandas as pd\n",
    "parquet_files=['book2.parquet','home2.parquet','personal_care2.parquet']\n",
    "dataframes=[pd.read_parquet(files) for files in parquet_files]\n",
    "df=pd.concat(dataframes,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labeling the response column with numerical value\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "df['response']=label_encoder.fit_transform(df['main_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing all numbers in the title columns: \n",
    "#import re\n",
    "#import string\n",
    "#def remove_punctuation(text):\n",
    "#    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "#def remove_num(text):\n",
    "#    text=remove_punctuation(text)\n",
    "#    return re.sub(r'\\d+','',text)\n",
    "\n",
    "#df['title']= df['title'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to lemmatize text - remove stop words\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting using TF-IDF \n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tdif=TfidfVectorizer(stop_words='english', preprocessor=lemmatize_text)  ## we can customize stop_words list as well . using default stop_words='english' has very minimal list. Instead we process stop_words and remove it using spacy  \n",
    "#X=tdif.fit_transform(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer2.pkl.gz']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## saving the vectorizer and resulting tf-dif matrix to a compressed file for future use. \n",
    "#from scipy.sparse import save_npz, load_npz\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import joblib\n",
    "\n",
    "#joblib.dump((tdif,X), 'tfidf_vectorizer2.pkl.gz',compress=('gzip',3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAmerica' 'AAndrea' 'AAngel' 'AAobosi' 'AArbutin' 'AArm' 'AAsXXX'\n",
      " 'AAtlonia' 'AAtter' 'AB' 'ABA' 'ABACAD' 'ABAKAN' 'ABALDI' 'ABALON'\n",
      " 'ABAMERICA' 'ABAO' 'ABAPA' 'ABARGK' 'ABASSKY' 'ABATE' 'ABB' 'ABBA' 'ABBB'\n",
      " 'ABBBED' 'ABBD' 'ABBETEY' 'ABBEY' 'ABBLE' 'ABBLU' 'ABBOT' 'ABBOTT'\n",
      " 'ABBOUD' 'ABBTO' 'ABBY' 'ABBYE' 'ABBYNEW' 'ABC' 'ABCActivating' 'ABCB'\n",
      " 'ABCCAMPING' 'ABCCANOPY' 'ABCCBADKAKGINGNHTNLHHTOEOGHTIGTTHHHHHH' 'ABCD'\n",
      " 'ABCDE' 'ABCDEF' 'ABCDEFG' 'ABCDEFZ' 'ABCDerm']\n"
     ]
    }
   ],
   "source": [
    "#print(tdif.get_feature_names_out()[101:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the vectorizer file from disk \n",
    "import joblib\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "v,X=joblib.load('tfidf_vectorizer2.pkl.gz')\n",
    "\n",
    "# Testing with new text data using the loaded vectorizer\n",
    "new_texts = [\"New data to transform.\"]\n",
    "new_tfidf_matrix = v.transform(new_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape  (3000000, 624602)\n"
     ]
    }
   ],
   "source": [
    "print(\"matrix shape \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression with 1 variable 'Title' - X is the embedding for title column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,df['response'],train_size=.8,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logmod=LogisticRegressionCV(multi_class='multinomial',cv=5, max_iter=10000000).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.65009125\n",
      "[[382334  50280 367379]\n",
      " [  5192 788651   5789]\n",
      " [360654  50487 389234]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "ytrain_pred=logmod.predict(x_train)\n",
    "log_train_matrix=confusion_matrix(y_train,ytrain_pred)\n",
    "print('train accuracy', accuracy_score(y_train,ytrain_pred))\n",
    "print(log_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.48      0.49    799993\n",
      "           1       0.89      0.99      0.93    799632\n",
      "           2       0.51      0.49      0.50    800375\n",
      "\n",
      "    accuracy                           0.65   2400000\n",
      "   macro avg       0.64      0.65      0.64   2400000\n",
      "weighted avg       0.64      0.65      0.64   2400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,ytrain_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.6049566666666667\n",
      "[[ 81957  12766 105284]\n",
      " [  1334 197588   1446]\n",
      " [103637  12559  83429]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.41      0.42    200007\n",
      "           1       0.89      0.99      0.93    200368\n",
      "           2       0.44      0.42      0.43    199625\n",
      "\n",
      "    accuracy                           0.60    600000\n",
      "   macro avg       0.59      0.60      0.60    600000\n",
      "weighted avg       0.59      0.60      0.60    600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytest_pred=logmod.predict(x_test)\n",
    "log_test_matrix=confusion_matrix(y_test,ytest_pred)\n",
    "print(\"test accuracy\",accuracy_score(y_test,ytest_pred))\n",
    "print(log_test_matrix)\n",
    "print(classification_report(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.41      0.42    200007\n",
      "           1       0.89      0.99      0.93    200368\n",
      "           2       0.44      0.42      0.43    199625\n",
      "\n",
      "    accuracy                           0.60    600000\n",
      "   macro avg       0.59      0.60      0.60    600000\n",
      "weighted avg       0.59      0.60      0.60    600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book2', 'home', 'personal_care'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LogisticModel_titlefeature_tdif.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## saving logistic models for future use\n",
    "logistic_model_file='LogisticModel_titlefeature_tdif.pkl'\n",
    "joblib.dump(logmod,logistic_model_file,compress=('gzip',3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for home product based on title gain a significant high accuracy while book and personalcare show intermixed results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
